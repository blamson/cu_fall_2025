---
title: "Hw06 - Regional Count Data Homework (Spatial Autocorrelation)"
format: html
self-contained: true
author: Brady Lamson
date: 10/22/2025
---

The `lip_sf` is an `sf` data frame with 53 rows representing counties and 5 variables. The data frame contains 53 county-level observations for lip cancer among males in Scotland between 1975-1980. The variables are:

-   `name`: county name
-   `cancer`: number of Lip cancer cases in the county (Cressie 1993)
-   `population`: county population size (Cressie 1993)
-   `expected`: expected number of lip cancer cases (Lawson 1999)
-   `geometry`: geometric representation of counties in Scotland.

Source Kemp I., Boyle P., Smans M. and Muir C. (1985) Atlas of cancer in Scotland, 1975-1980, incidence and epidemiologic perspective. *International Agency for Research on Cancer*. 72

Cressie, N. A. C. (1993). Statistics for Spatial Data. New York: John Wiley & Sons, p. 537 Table 7.2.

Lawson, A., Biggeri, A., Böhning, D., Lesaffre, E., Viel, J. F., & Bertollini, R. (Eds.). (1999). Disease Mapping and Risk Assessment for Public Health. New York: Wiley, pp. 68-69, Table 5.1.

We run the following command to load `lip_sf` (assuming the `.rda` is in your search path).

```{r}
load("lip_sf.rda")
```

We run the following commands to geometrically obtain centroids for each county and create a neighborhood object for the data.

```{r}
#| include: false
library(sf)
library(smerc)
library(spdep)
```

```{r}
coords <- st_coordinates(st_centroid(st_geometry(lip_sf)))
lip_nb <- poly2nb(lip_sf, queen=TRUE)
```

# Problem 1

Plot the neighborhood connections stored in `lip_nb`. What do you observe?

**Solution**

```{r}
# plot(coords)
plot(st_geometry(lip_sf), border="grey60")
plot(lip_nb, coords = coords, add=TRUE, pch=19, cex=0.6, col="blue")
```

The southern half of scotland is very interconnected, all of the regions down there have multiple neighbors. The regions in the top half of the map have less direct neighbors in comparison.

# Problem 2

Use the Moran’s I statistic to test whether there is evidence of positive spatial autocorrelation for the `cancer` variable under the constant risk hypothesis. Use a row standardized weights matrix for the `lip_nb` neighbor relationship. Use the `expected` counts stored in `lip_sp` for the expected counts. Provide the observed test statistic, the p-value, and interpret your results in the context of the problem.

**Solution**

```{r}
# Style = "W" for weight standardization
lw <- nb2listw(lip_nb, style="W")
moran.test(lip_sf$expected, listw = lw, randomisation = FALSE)
```

$$
H_0: \rho = 0
$$

$$
H_a: \rho > 0 
$$

The p-value of this test is $0.3554 > 0.05$, so there isn't significant evidence of positive autocorrelation under the constant risk hypothesis. In other words, these results tell us that there isn't evidence to indicate that areas with higher than expected cancer cases are more likely to be near other areas with higher than expected cancer cases.

# Problem 3

Use the constant risk version of Moran’s I (Walter 1992) to test whether there is evidence of positive spatial autocorrelation for the `cancer` variable under the CRH. Use a row binary weights matrix for the `lip_nb` neighbor relationship. Use the `expected` counts stored in `lip_sp` for the expected counts. Interpret your results in the context of the problem.

**Solution**

```{r}
# Style = "B" for binary
lw <- nb2listw(lip_nb, style="B")
moran.test(lip_sf$expected, listw = lw, randomisation = FALSE)
```

These results reach the same conclusion as part 2. This test has the same null and alternative hypotheses, it just uses a different method of encoding the data. 

So, neither method of encoding the regional neighbors in scotland showed significant evidence of positive autocorrelation of cancer cases. So we can't conclude that just because a region has higher than expected cancer cases that its neighbors will show the same behavior. 

# Problem 4

The intercentroid distances for the North Carolina data are between 4866.044 and 396,350.916. In the context of Tango’s recommended weights matrix, a very weak spatial correlation $\kappa=5000$ and a very strong spatial correlation has $\kappa=200000$. Perform Monte Carlo tests using using Tango's index with Tango's recommended weights with both $\kappa=5000$ and $\kappa=200000$ for the `cancer` variable with 499 simulated data sets. Interpret your results in the context of the problem.

**Solution**

```{r}
w5k <- dweights(coords, kappa=5000)
w200k <- dweights(coords, kappa=200000)

(tango_mc5k <-  tango.test(lip_sf$cancer, lip_sf$population, w5k, nsim = 499))
(tango_mc200k <-  tango.test(lip_sf$cancer, lip_sf$population, w200k, nsim = 499))
```

These results ($p = 0.002 < 0.05$) show that the observed cancer proportions in the various regions of scotland are inconsistent with what we would expect under the constant risk hypothesis. These results are consistent across both values of kappa. In other words, there is significant evidence of clustering of cancer cases 

# Problem 5

Compare the goodness-of-fit and spatial autocorrelation components of Tango’s statistic for the observed and simulated data in a plot. (Do this for both values of $\kappa$). Are the patterns similar for the weak versus strong spatial autocorrelation? Or does the value of $\kappa$ dramatically impact the relative importance of the goodness-of-fit and spatial autocorrelation components?

**Solution**

**kappa = 5000 plots**

```{r}
hist(tango_mc5k$gof.sim, xlim = range(c(tango_mc5k$gof.sim, tango_mc5k$gof)))
abline(v = tango_mc5k$gof)
```

```{r}
hist(tango_mc5k$sa.sim, xlim = range(c(tango_mc5k$sa.sim, tango_mc5k$sa)))
abline(v = tango_mc5k$sa)
```

**kappa = 200k plots**

```{r}
hist(tango_mc200k$gof.sim, xlim = range(c(tango_mc200k$gof.sim, tango_mc200k$gof)))
abline(v = tango_mc200k$gof)
```

```{r}
hist(tango_mc200k$sa.sim, xlim = range(c(tango_mc200k$sa.sim, tango_mc200k$sa)))
abline(v = tango_mc200k$sa)
```

**Conclusion**

The plots here look pretty consistent across both values of kappa. Both the goodness of fit and autocorrelation components for the observed values fall well outside the distributions of the simulated data. This leads me to believe that in this instance isn't having a dramatic impact on the relative importance of these components. 

# Problem 6

In this problem you are going to implement a portion of the spatial scan method. You can only use functions/packages loaded with by default by R (`stats`, `graphics`, `grDevices`, `utils`, `datasets`, `methods`, `base`). If you don't have to load a package to access the functionality, then you should be okay.

Suppose you have regional count data with the following characteristics:

```{r}
#| include: false
region_id <- seq_len(4)
x <- c(1, 3, 2, 0)
y <- c(2, 1, 1.5, 2)
cases <- c(1, 3, 3, 4)
pop <- c(2, 4, 6, 4)
dtf <- data.frame(region_id, x, y, cases, population = pop)
```

```{r}
#| echo: false
knitr::kable(dtf)
```

(`x`, `y`) define the centroid of each region.

Calculate the Poisson spatial scan statistic under the CRH assuming the constraint that no more than half the total population can be in a potential cluster/window.

Break up your solution into parts:

## (a)

Compute the inter-centroid distance matrix between all centroids.

Return the sample mean of this matrix.

**Solution**

We can just use the base R distance function for this. We take the mean BEFORE the matrix function so we don't include the 0 diagonals. 

```{r}
coords = as.matrix(dtf[,c("x", "y")])

d <- dist(coords)
print(mean(d)) # Mean of just the lower triangle
d = as.matrix(d)
```

## (b)

Using the distance matrix above, determine all possible windows (in terms of the region ids each window includes) in terms of nearest neighbors (the largest would have 3 non-inclusive neighbors). Print the complete list of windows.

**Solution**

Kinda goal output here:
[[1]] 1
[[2]] 1 3
[[3]] 1 3 2
[[4]] 1 3 2 4
[[5]] 2
[[6]] 2 3
[[7]] 2 3 1
[[8]] 2 3 1 4
...

```{r}
windows <- list()
windows_idx <- 1

# Go through each row, order em by distance and then
# iteratively add each nearest neighbor to the candidate windows
for (i in 1:nrow(d)) {
    ordered_row <- order(d[i,]) # gives INDICES not values.

    for (j in 1:length(ordered_row)) {
        j_nn <- ordered_row[1:j] # Collect up to 'j' nearest neighbors
        windows[[windows_idx]] <- j_nn
        windows_idx <- windows_idx + 1
    }
}

print(windows)
```


## (c)

Determine the population size of each window to identify which windows have less than 50% of the total population. Print the population of each window

**Solution**

Let's modify the previous sections code a bit to capture this info. 

```{r}
total_pop <- sum(dtf$population)

windows <- list()
window_populations <- c()
windows_idx <- 1

# Go through each row, order em by distance and then
# iteratively add each nearest neighbor to the candidate windows
for (i in 1:nrow(d)) {
    ordered_row <- order(d[i,]) # gives INDICES not values.

    for (j in 1:length(ordered_row)) {
        j_nn <- ordered_row[1:j] # Collect up to 'j' nearest neighbors
        window_pop <- sum(dtf$population[c(j_nn)])
        window_populations[windows_idx] <- window_pop
        # print(window_pop)
        windows[[windows_idx]] <- j_nn
        windows_idx <- windows_idx + 1
    }
}

# Sorry for the huge output!
for (i in 1:length(windows)) {
    print(paste0("Window: ", windows[i]))
    print(paste0("Population: ", window_populations[i]))
    print("---")
}
```


## (d)

Only retain the windows that satisfy the population constraint. Print the list of retained windows.

**Solution**

```{r}
pop_constraint <- total_pop / 2
retained_windows_idx <- which(window_populations <= pop_constraint)
retained_windows <- windows[retained_windows_idx]
retained_populations <- window_populations[retained_windows_idx]

for (i in 1:length(retained_windows)) {
    print(paste0("Window: ", retained_windows[i]))
    print(paste0("Population: ", retained_populations[i]))
    print("---")
}
```


## (e)

For each remaining window, compute $Y_{in}$, $Y_{out}$, $E_{in}$, $E_{out}$. Print a data frame/matrix with the columns $Y_{in}$, $Y_{out}$, $E_{in}$, $E_{out}$.

**Solution**

- yin is number of cases in the window
- ein is r * population in the window
- yout number of cases everywhere else
- eout r * population outside the window
- assume r is the overall rate of cases

Really not a lot to say here. We simply collect the information we need. Summing up and scaling the necessary rows from the original dataset. 

```{r}
r <- sum(dtf$cases) / sum(dtf$population)
n <- length(retained_windows)
window_df <- data.frame(
    regions = I(retained_windows),
    yin = numeric(n),
    ein = numeric(n),
    yout = numeric(n),
    eout = numeric(n)
)

for (i in 1:length(retained_windows)) {
    window_df$ein[i] <- r * sum(dtf[retained_windows[[i]], "population"])
    window_df$yin[i] <- sum(dtf[retained_windows[[i]], "cases"])
    window_df$yout[i] <- sum(dtf[-retained_windows[[i]], "cases"])
    window_df$eout[i] <- r * sum(dtf[-retained_windows[[i]], "population"])
}

window_df
```


## (f)

Compute the statistic $\left(\frac{Y_{in}}{E_{in}}\right)^{Y_{in}} \left(\frac{Y_{out}}{E_{out}}\right)^{Y_{out}} I\left(\frac{Y_{in}}{E_{in}} \geq \frac{Y_{out}}{E_{out}}\right)$ for each remaining window.

**Solution**

I believe the below code is pretty self explanatory. We just apply this formula to every row in the windows dataframe! 

```{r}
window_df$test_stat <- apply(window_df, 1, function(row) {
    yin <- as.numeric(row["yin"])
    ein <- as.numeric(row["ein"])
    yout <- as.numeric(row["yout"])
    eout <- as.numeric(row["eout"])
    
    in_frac <- yin / ein
    out_frac <- yout / eout
    check <- in_frac >= out_frac
    
    if (check) {
        return((in_frac ^ yin) * (out_frac ^ yout))
    } else {
        return(0)
    }
})

window_df
```

And with that we're done. 

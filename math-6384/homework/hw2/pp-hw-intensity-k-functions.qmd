---
title: "Point Process Homework (Intensity and K Functions)"
format: html
self-contained: true
author: Brady Lamson
date: 09-03-2025
---

```{r, message=FALSE, results=FALSE, warning=FALSE, echo=FALSE}
packages <- c("spatstat", "pbapply", "smacpod")

for (pkg in packages) {
  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cran.rstudio.com/")
    library(pkg, character.only = TRUE)
  }
}
```

Instructions:  Answer the following questions and write your answers in a word processor.  Mathematical symbols should be written using the equation editor.  Appropriate graphics should be included.  The document may also be created in LaTeX, though this is NOT encouraged.

# Problem 1 

The **spatstat** package includes a data set named `japanesepines`.  The data give the locations of Japanese black pine saplings in a square sampling region in a natural forest. The observations were originally collected by Numata (1961).  



```{r}
pines <- spatstat.data::japanesepines
```

## A

**Problem:**

Construct a plot of $\hat{L}(h)-h$ versus $h$ for the data with 95% tolerance envelopes.  Use $N_{sim} = 499$ simulated data sets to construct the envelopes. You do not need to specify `r` (the vector of distances) in the function; simply use the default values.

**Answer:**

```{r, results='hide'}
#| code-fold: true
#| code-summary: The code

lplot <- function(x, nsim = 499, level = 0.95,
                  correction = "Ripley", ...) {
  e_outer = envelope(x, fun = spatstat.explore::Lest,
                     nsim = nsim, nrank = 1,
                     savepatterns = TRUE)
  e_tol = envelope(e_outer, fun = spatstat.explore::Lest,
                   nsim = nsim,
                   nrank = floor((1 - level) * (nsim + 1)/2))
  plot(e_outer, fmla = . - r ~ r, legend = FALSE, main = "")
  plot(e_tol, fmla = . - r ~ r, shadecol = "lightgrey", add = TRUE)
}

lplot(pines)
```
## B

**Problem:**

Using the previous plot, what do you conclude about the data's compability with CSR? Are there scales for which the event locations seem to exhibit more clustering than expected for a CSR process? Or more regularity than expected for a CSR process?

**Answer:**

The plot shows the majority of the curve is under 0. This would indicate *some* evidence of regularity in the data relative to CSR at all the scales shown. However, the line always falls comfortably within the $95\%$ envelopes. What this means is that there isn't strong enough evidence to conclude that this data isn't compatible with CSR. 

## C

**Problem:**

Perform a hypothesis test to determine whether there is evidence of a departure from CSR by the data set at a significance level of 0.05 at some spatial scale between 0 and 0.25. Use $N_{sim} = 499$ simulated data sets to compute your Monte Carlo p-values. What is your conclusion?

**Solution:**

Okay so to break things down a bit. We want to get $T_{obs} = \max(|\hat{L}(h) - h|)$ for our observed data and compare it to simulated CSR data in the same study area. $h$ in this case is a range of distances we're curious about looking into. For this problem that is $[0, 0.25]$. 

We generate an arbitrary amount of simulations, 499 in this problem, and then do the same computation for each of our simulated datasets. From there we look at how many times the data we observed and simulated was at least as extreme as the data we observed. 

To simulate our data we need the study area from the pines dataset and the intensity of that dataset. Thankfully those are both things `spatstat` has functions for.

To really remember *what* we're doing here, we're seeing if there's any evidence of departure from CSR anywhere along $h$. That's why we look at the maximum of $|\hat{L}(h) - h|$. We flatten things down to just examine the most extreme deviation to avoid the multiple comparisons problem. The absolute value is there to make sure we're capturing instances of regularity and clustering. 

```{r, results='hide', warning=FALSE}
#| code-fold: true
#| code-summary: The code

set.seed(500)

# Break out \hat{L}(h) just for readability
L_obs <- spatstat.explore::Lest(pines, correction = "Ripley")
# Calc observed test statistic across h
Tobs <- max( abs(L_obs$iso - L_obs$r) )

# Do the same setup for the simulated datasets.
Tsim <- pbapply::pbsapply(1:499, FUN = function(i) {
    sim_data <- spatstat.random::rpoispp(
        lambda=spatstat.geom::intensity(pines), 
        win = spatstat.geom::Window(pines)
    )
    sim_L <- spatstat.explore::Lest(sim_data, correction = "Ripley")
    max( abs(sim_L$iso - sim_L$r) )
})

# We make sure to include our observed data in the p-value. 
p <- mean(c(Tsim, Tobs) >= Tobs)
```

```{r, echo=FALSE}
cat("P-Value:", p)
```

```{r}
#| code-fold: true
#| code-summary: Make a plot for my own sake

plot(density(Tsim), 
     xlim = range(c(Tsim, Tobs)),
     main = "Monte Carlo Test Statistic Distribution",
     xlab = "T")
abline(v = Tobs, col = "red", lty = 2, lwd = 2)
```

Note: The red vertical line represents $T_{obs}$.

**Conclusion:**

This test resulted in a p-value of $0.696$. These results allow us to conclude that there is not significant evidence of a departure from CSR for the pines data for $0 \leq h \leq 0.25$. 

# Problem 2

Repeat the analysis from problem 1 for the `redwood` data in the **spatstat** package. The data represent the locations of 62 seedlings and saplings of California redwood trees in a square sampling region. They originate from Strauss (1975); the present data are a subset extracted by Ripley (1977) in a subregion that has been rescaled to a unit square.  The coordinates are rounded to the nearest 0.01 units, except for one point which has an x coordinate of 0.999, presumably to ensure that it is properly inside the window.

```{r}
redwood <- spatstat.data::redwood
```


## A 

**Problem:**

Plot the data.  Comment on what you see and whether this looks compatible with CSR.

**Solution:**

```{r}
plot(redwood)
```

I wouldn't say this data looks compatible with CSR. There's like this clustered S shape in the data. A lot of the gaps don't look like they're there due to random chance. This plot looks very clustered. 

## B

**Problem:**

Estimate and create a contour plot of the intensity function (use Scottâ€™s rule for the bandwidth in each direction).  Comment on the plot in relation to potential departures from CSR.

**Solution:**

```{r}
bandwidth <- spatstat.explore::bw.scott(redwood)
intensity_est <- smacpod::spdensity(redwood, sigma = bandwidth)
contour(intensity_est)
```

We see a number of distinct local maxes here. There's 2 in the bottom left and one in the top right. We also see some huge dips like in the top left and middle right. Also of note is right in the middle is a small dip between the local maxes that isn't actually that insane of a drop. It follows that S shape I described earlier. I feel like all of that is evidence of some sort of clustering and a departure from CSR. 

## C 

**Problem:**

Construct a plot of $\hat{L}(h)-h$ versus $h$ for the data with 95% tolerance envelopes.  Use $N_{sim} = 499$ simulated data sets to construct the envelopes. You do not need to specify `r` (the vector of distances) in the function; simply use the default values.

**Solution:**

```{r, results='hide', warning=FALSE}
lplot(redwood)
```


## D 

**Problem:**

Using the previous plot, what do you conclude about the data's compability with CSR? Are there scales for which the event locations seem to exhibit more clustering than expected for a CSR process? Or more regularity than expected for a CSR process?

**Solution:**

There seems to be a range of spatial scales where clustering may be happening. Around $0.05$ to $0.15$ is well above the confidence envelopes for the simulated data. I would say that for those scales we're seeing a good bit of evidence of clustering. 

## E

**Problem:**

Perform a test to determine whether there evidence of a departure from CSR by the data set at a significance level of 0.05 at some spatial scale between 0 and 0.25. What is your conclusion? Use $N_{sim} = 499$ simulated data sets to compute your Monte Carlo p-values.

**Solution:**

```{r, results='hide', warning=FALSE}
#| code-fold: true
#| code-summary: Exact same code as last time.

set.seed(500)

# Break out \hat{L}(h) just for readability
L_obs <- spatstat.explore::Lest(redwood, correction = "Ripley")
# Calc observed test statistic across h
Tobs <- max( abs(L_obs$iso - L_obs$r) )

# Do the same setup for the simulated datasets.
Tsim <- pbapply::pbsapply(1:499, FUN = function(i) {
    sim_data <- spatstat.random::rpoispp(
        lambda=spatstat.geom::intensity(redwood), 
        win = spatstat.geom::Window(redwood)
    )
    sim_L <- spatstat.explore::Lest(sim_data, correction = "Ripley")
    max( abs(sim_L$iso - sim_L$r) )
})

# We make sure to include our observed data in the p-value. 
p <- mean(c(Tsim, Tobs) >= Tobs)
```

```{r, echo=FALSE}
cat("P-Value:", p)
```

Our results show that $p=0.002<\alpha$. This allows us to conclude that there is significant evidence of a departure from CSR at some spatial scale between $0 \leq h \leq 0.25$. 

# Problem 3

Run the following code to generate some event locations in one-dimension.

```{r}
set.seed(1)
x <- runif(15) # runif defaults to min=0, max=1
```

Now define `h` in the following way:

```{r}
h <- seq(-1, 2, len = 1000)
```

We will now visualize the process of density estimation for two different bandwidths. Note that we are technically not constructing densities because we are not scaling correctly.

## A 

**Problem:** Complete the following tasks.

Tasks:

    iii. For each event location, evaluate the Gaussian kernel function (this is simply `dnorm`) at each value of `h` using the event location as the `mean` argument and with a `sd` argument of 0.10. 
    
    ii. For each value of `h`, sum the evaluated kernel values across all 15 event locations.
    
    iii. Create a plot that displays the sum of the evaluated kernel functions on the y-axis and the value of `h` on the x-axis. On this plot, overlay the evaluated kernel function for each event location as a function of `h`.

**Solution:**

```{r}
#| code-fold: true
#| code-summary: Code

# (i) Evaluate Gaussian kernel (sd = 0.1) at each event location
# Object is 1000 rows by 15 columns
kernels <- sapply(x, function(xi) dnorm(h, mean = xi, sd = 0.10)) 

# (ii) Sum across rows
kernel_sums <- rowSums(kernels)

plot(x=h, y=kernel_sums, type="l", lwd=2)
# Use matlines here for easily showing all of the lines on the plot at once without calling lines() a ton. 
matlines(h, kernels, lty = 1, col = "darkgrey")
```

## B 

**Problem:**

Repeat the same process as (a) using a `sd`/bandwidth of 0.25.

**Solution:**

```{r}
#| code-fold: true
#| code-summary: Same code
#| 
kernels <- sapply(x, function(xi) dnorm(h, mean = xi, sd = 0.25)) 
kernel_sums <- rowSums(kernels)

plot(x=h, y=kernel_sums, type="l", lwd=2)
matlines(h, kernels, lty = 1, col = "darkgrey")
```

## C

**Problem:**

What is the relationship between bandwidth and the smoothness of the estimated density?

**Solution:**

The larger bandwidth completely smooths out the line into a nice bell curve. The small bandwidth shows more of the local humps. We can even see this in the location specific curves. The smaller bandwidth means their overlap is smaller. The larger has the curves overlapping so much more. So a larger bandwidth smooths out the estimated density as a result.